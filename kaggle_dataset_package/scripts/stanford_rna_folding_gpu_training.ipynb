{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford RNA 3D Folding - GPU Training with Physics-Enhanced Transformer\n",
    "\n",
    "This notebook trains a physics-enhanced transformer model for RNA 3D structure prediction using Kaggle's GPU resources.\n",
    "\n",
    "## Model Features\n",
    "- Physics-enhanced transformer with biophysical constraints\n",
    "- Single-atom coordinate prediction (x_1, y_1, z_1)\n",
    "- RMSD and TM-score evaluation metrics\n",
    "- Bond length, bond angle, and steric clash constraints\n",
    "- GPU-optimized training with mixed precision\n",
    "\n",
    "## Expected Performance\n",
    "- 20-50x speedup over CPU training\n",
    "- Training time: 2-4 hours for full model\n",
    "- Target metrics: RMSD < 5.0 Ã…, TM-score > 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install packages that might not be in Kaggle environment\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    install_package(\"PyYAML\")\n",
    "    import yaml\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "except ImportError:\n",
    "    install_package(\"scikit-learn\")\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load RNA Folding Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset source code to path\n",
    "dataset_path = \"/kaggle/input/stanford-rna-3d-folding\"\n",
    "if os.path.exists(dataset_path):\n",
    "    sys.path.insert(0, dataset_path)\n",
    "    print(f\"Added dataset path: {dataset_path}\")\n",
    "else:\n",
    "    print(\"Warning: Dataset path not found. Please ensure the dataset is attached.\")\n",
    "    # List available datasets\n",
    "    input_dir = Path(\"/kaggle/input\")\n",
    "    if input_dir.exists():\n",
    "        print(\"Available datasets:\")\n",
    "        for item in input_dir.iterdir():\n",
    "            print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kaggle training script\n",
    "kaggle_script_path = f\"{dataset_path}/scripts/rna_folding_kaggle_train.py\"\n",
    "\n",
    "if os.path.exists(kaggle_script_path):\n",
    "    print(\"Loading Kaggle training script...\")\n",
    "    exec(open(kaggle_script_path).read())\n",
    "    print(\"Training script loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Training script not found at: {kaggle_script_path}\")\n",
    "    print(\"Available files in scripts directory:\")\n",
    "    scripts_dir = Path(f\"{dataset_path}/scripts\")\n",
    "    if scripts_dir.exists():\n",
    "        for file in scripts_dir.iterdir():\n",
    "            print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Performance Analysis and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GPU capabilities and recommend configuration\n",
    "def analyze_gpu_performance():\n",
    "    \"\"\"Analyze GPU and recommend optimal training configuration.\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        return {\n",
    "            \"device\": \"cpu\",\n",
    "            \"recommended_batch_size\": 4,\n",
    "            \"use_mixed_precision\": False,\n",
    "            \"expected_speedup\": \"1x (baseline)\"\n",
    "        }\n",
    "    \n",
    "    device_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_name = device_props.name\n",
    "    gpu_memory_gb = device_props.total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU Analysis:\")\n",
    "    print(f\"  Name: {gpu_name}\")\n",
    "    print(f\"  Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    print(f\"  Compute Capability: {device_props.major}.{device_props.minor}\")\n",
    "    \n",
    "    # Recommend configuration based on GPU\n",
    "    if \"V100\" in gpu_name or gpu_memory_gb >= 30:\n",
    "        config = {\n",
    "            \"device\": \"cuda\",\n",
    "            \"recommended_batch_size\": 48,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"learning_rate_multiplier\": 6.0,\n",
    "            \"use_mixed_precision\": True,\n",
    "            \"expected_speedup\": \"75-125x\",\n",
    "            \"estimated_training_time\": \"1-1.5 hours\"\n",
    "        }\n",
    "    elif \"P100\" in gpu_name or gpu_memory_gb >= 15:\n",
    "        config = {\n",
    "            \"device\": \"cuda\",\n",
    "            \"recommended_batch_size\": 24,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"learning_rate_multiplier\": 3.0,\n",
    "            \"use_mixed_precision\": True,\n",
    "            \"expected_speedup\": \"40-75x\",\n",
    "            \"estimated_training_time\": \"1.5-2.5 hours\"\n",
    "        }\n",
    "    elif \"T4\" in gpu_name or gpu_memory_gb >= 12:\n",
    "        config = {\n",
    "            \"device\": \"cuda\",\n",
    "            \"recommended_batch_size\": 16,\n",
    "            \"gradient_accumulation_steps\": 3,\n",
    "            \"learning_rate_multiplier\": 2.0,\n",
    "            \"use_mixed_precision\": True,\n",
    "            \"expected_speedup\": \"25-50x\",\n",
    "            \"estimated_training_time\": \"2-4 hours\"\n",
    "        }\n",
    "    else:\n",
    "        config = {\n",
    "            \"device\": \"cuda\",\n",
    "            \"recommended_batch_size\": 12,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "            \"learning_rate_multiplier\": 1.5,\n",
    "            \"use_mixed_precision\": True,\n",
    "            \"expected_speedup\": \"15-30x\",\n",
    "            \"estimated_training_time\": \"3-6 hours\"\n",
    "        }\n",
    "    \n",
    "    print(f\"\\nRecommended Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "gpu_config = analyze_gpu_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the RNA dataset\n",
    "def analyze_rna_dataset(data_dir):\n",
    "    \"\"\"Analyze RNA dataset and display statistics.\"\"\"\n",
    "    \n",
    "    data_path = Path(data_dir) / \"data\"\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Data directory not found: {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Analyze each dataset file\n",
    "    for file_name in [\"train_sequences.csv\", \"train_labels.csv\", \"validation_sequences.csv\", \"validation_labels.csv\"]:\n",
    "        file_path = data_path / file_name\n",
    "        \n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            analysis[file_name] = {\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": list(df.columns),\n",
    "                \"size_mb\": file_path.stat().st_size / (1024 * 1024)\n",
    "            }\n",
    "            \n",
    "            # Special analysis for sequences\n",
    "            if \"sequence\" in df.columns:\n",
    "                sequences = df[\"sequence\"].dropna()\n",
    "                analysis[file_name].update({\n",
    "                    \"avg_length\": sequences.str.len().mean(),\n",
    "                    \"max_length\": sequences.str.len().max(),\n",
    "                    \"min_length\": sequences.str.len().min()\n",
    "                })\n",
    "            \n",
    "            # Special analysis for coordinates\n",
    "            coord_cols = [col for col in df.columns if col.startswith(('x_', 'y_', 'z_'))]\n",
    "            if coord_cols:\n",
    "                analysis[file_name][\"coordinate_columns\"] = len(coord_cols)\n",
    "                analysis[file_name][\"nan_coordinates\"] = df[coord_cols].isna().sum().sum()\n",
    "    \n",
    "    # Display analysis\n",
    "    print(\"Dataset Analysis:\")\n",
    "    for file_name, stats in analysis.items():\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(f\"  Rows: {stats['rows']:,}\")\n",
    "        print(f\"  Size: {stats['size_mb']:.2f} MB\")\n",
    "        \n",
    "        if \"avg_length\" in stats:\n",
    "            print(f\"  Avg sequence length: {stats['avg_length']:.1f}\")\n",
    "            print(f\"  Max sequence length: {stats['max_length']}\")\n",
    "        \n",
    "        if \"coordinate_columns\" in stats:\n",
    "            print(f\"  Coordinate columns: {stats['coordinate_columns']}\")\n",
    "            print(f\"  NaN coordinates: {stats['nan_coordinates']:,}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_analysis = analyze_rna_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized training configuration\n",
    "def create_training_config(gpu_config, dataset_path):\n",
    "    \"\"\"Create training configuration optimized for detected GPU.\"\"\"\n",
    "    \n",
    "    base_lr = 5e-4\n",
    "    lr_multiplier = gpu_config.get(\"learning_rate_multiplier\", 1.0)\n",
    "    \n",
    "    config = {\n",
    "        # Experiment info\n",
    "        \"experiment_name\": \"kaggle_rna_folding_gpu\",\n",
    "        \"project\": \"stanford-rna-3d-folding\",\n",
    "        \"seed\": 42,\n",
    "        \n",
    "        # Paths\n",
    "        \"data_dir\": f\"{dataset_path}/data\",\n",
    "        \"save_dir\": \"/kaggle/working/experiments\",\n",
    "        \"checkpoint_dir\": \"/kaggle/working/checkpoints\",\n",
    "        \n",
    "        # GPU-optimized training\n",
    "        \"device\": gpu_config[\"device\"],\n",
    "        \"batch_size\": gpu_config[\"recommended_batch_size\"],\n",
    "        \"use_mixed_precision\": gpu_config[\"use_mixed_precision\"],\n",
    "        \"gradient_accumulation_steps\": gpu_config.get(\"gradient_accumulation_steps\", 1),\n",
    "        \n",
    "        # Learning parameters\n",
    "        \"learning_rate\": base_lr * lr_multiplier,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"epochs\": 25,  # Reduced for Kaggle time limits\n",
    "        \"patience\": 10,\n",
    "        \"min_epochs\": 5,\n",
    "        \n",
    "        # Model architecture\n",
    "        \"embedding_dim\": 256,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_heads\": 8,\n",
    "        \"dropout\": 0.1,\n",
    "        \"num_atoms\": 1,\n",
    "        \"multi_atom_mode\": False,\n",
    "        \"coord_dims\": 3,\n",
    "        \"max_seq_len\": 1200,\n",
    "        \"use_relative_attention\": False,\n",
    "        \"use_rna_constraints\": True,\n",
    "        \n",
    "        # Physics constraints\n",
    "        \"bond_length_weight\": 0.3,\n",
    "        \"bond_angle_weight\": 0.3,\n",
    "        \"steric_clash_weight\": 0.5,\n",
    "        \"watson_crick_weight\": 0.2,\n",
    "        \"normalize_coords\": True,\n",
    "        \n",
    "        # Optimization\n",
    "        \"scheduler_type\": \"reduce_on_plateau\",\n",
    "        \"lr_factor\": 0.5,\n",
    "        \"lr_patience\": 5,\n",
    "        \"min_lr\": 1e-7,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \n",
    "        # Data loading\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": True,\n",
    "        \n",
    "        # Checkpointing\n",
    "        \"keep_last_n_checkpoints\": 3,\n",
    "        \"save_every_n_epochs\": 5,\n",
    "        \n",
    "        # Kaggle specific\n",
    "        \"kaggle_mode\": True,\n",
    "        \"disable_wandb\": True\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "training_config = create_training_config(gpu_config, dataset_path)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with monitoring\n",
    "print(\"=== Starting RNA Folding Model Training ===\")\n",
    "print(f\"Expected training time: {gpu_config.get('estimated_training_time', 'Unknown')}\")\n",
    "print(f\"Expected speedup: {gpu_config.get('expected_speedup', 'Unknown')}\")\n",
    "\n",
    "# Record start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Run training (this will use the main() function from the loaded script)\n",
    "    model, results = main()\n",
    "    \n",
    "    # Record end time\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(f\"\\n=== Training Completed Successfully ===\")\n",
    "    print(f\"Training time: {training_duration/3600:.2f} hours\")\n",
    "    print(f\"Best RMSD: {results['best_rmsd']:.4f}\")\n",
    "    print(f\"Best TM-score: {results['best_tm_score']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }\n",
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}
